{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1664bdf6-8f0f-4621-b6b6-9bc48a790985",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Team 6-1\n",
    "\n",
    "| Name             | E-mail                     | Photo       |\n",
    "|------------------|----------------------------|-------------|\n",
    "| Jason Rudianto   | jasonrudianto@berkeley.edu | <img src=https://dchansew.github.io/w261/Team_6-1_Jason_Rudianto.jpg width=\"200\"> |\n",
    "| Derrick Chan-Sew | dchansew@berkeley.edu      | <img src=https://dchansew.github.io/w261/Team_6-1_Derrick_Chan-Sew.jpg width=\"200\"> |\n",
    "| Sean Condon      | scondon@berkeley.edu       | <img src=https://dchansew.github.io/w261/Team_6-1_Sean_Condon.png width=\"200\"> |\n",
    "| Ryan Chen        | bread12035@berkeley.edu    | <img src=https://dchansew.github.io/w261/Team_6-1_Ryan_Chen.png width=\"200\"> |\n",
    "| Jessica Huber    | jesshuber@berkeley.edu     | <img src=https://dchansew.github.io/w261/Team_6-1_Jessica_Huber.jpg width=\"200\">|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a386752c-3931-4a39-8cfd-c71eabd3d26f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Project Abstract\n",
    "\n",
    "The primary goals of this project phase are to complete exploratory data analysis, finalize and engineer more input features for delay prediction, and build out advanced machine learning pipelines for training and evaluation. With 5 years of airline and weather data and several ML models, we predict departure delays to reduce travel inconveniences for passengers, and reduce operational costs for airlines. In this phase, we added RFM, time-based, and event-based engineered features. Experimenting with Logistic Regression Classifiers, Decision Tree models, and neural networks, and taking a close look at information leakage and imbalanced data. We achieve an optimal accuracy of 82%, recall of 82% and precision of 70% with Multi-layer Perceptron (MLP). \n",
    "\n",
    "<img src=\"https://dchansew.github.io/w261/Phase 3 - Abstract.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35ef9ef7-1988-4a2d-b7a5-094ddb0b92ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data and Feature Engineering\n",
    "\n",
    "The data used in the project is a join between Airline On-Time Performance Data (Flights) from the Bureau of Transportation and weather station data sourced from the National Centers for Environmental Information. It represents the weather conditions collected by weather stations within proximity of departing flights at airports and detailed flight information. \n",
    "\n",
    "Phase 3 focuses on model selection, model tuning, and model evaluation using the full set of 5 years of weather & flight data. In the coming slides, we discuss newly engineered RFM, events-based, and time-based features, compare a handful of tuned ML classification models, and evaluate them with multiple performance metrics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab10c00d-9a08-4bc5-8b96-df32acf3f567",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##  Flight Data\n",
    "\n",
    "There are a number of key features in analyzing.\n",
    "- DEP_DEL - (Numerical) Difference in minutes between the scheduled and actual departure time.\n",
    "- DEP_DEL15 - (Binary) Whether the flight has been delayed for more than 15 minutes.\n",
    "- DAY_OF_WEEK - (Categorical) Day of the week of the scheduled flight.\n",
    "- OP_CARRIER_FL_NUM - (Categorical) ID of the flight number.\n",
    "- CRS_DEP_TIME - (Numerical) Scheduled departure time of the flight. \n",
    "- ORIGIN_AIRPORT_SEQ_ID - (Categorical) ID for an airport. Is numerical, but treated as a categorical feature.\n",
    "- OP_CARRIER - (Categorical) ID corresponding to the carrier of the flight\n",
    "- DISTANCE - (Numerical) Distance between the origin and destination airport in miles.\n",
    "- ORIGIN_STATE - (Categorical) Abbreviation the state of the origin airport \n",
    "- DEST_STATE - (Categorical) representing the state of the destination airport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b28533e-491b-44ed-914c-536346ceeb1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Weather Data\n",
    "\n",
    "Numerical features:\n",
    "\n",
    "- HourlyAltimeterSetting - Atmospheric pressure reduced to sea level, in inches of Mercury (in Hg).\n",
    "- HourlyDewPointTemperature - Dew point temperature (in whole degrees Fahrenheit)\n",
    "- HourlyDryBulbTemperature - Dry-bulb temperature, commonly used as the standard air temperature reported. Given in whole degrees Fahrenheit.\n",
    "- HourlyPrecipitation - Amount of precipitation in inches to hundredths over the past hour.\n",
    "- HourlyPressureChange - Difference in pressure over the past 3 hours. Prefixed with a “+” or “-” to indicate increase or decrease in pressure.\n",
    "- HourlyPressureTendency - Pressure tendency (0-3 indicates an increase in pressure, 5-8 indicates a decrease, 4 indicates no change)\n",
    "- HourlyRelativeHumidity - Relative humidity given to the nearest whole percentage.\n",
    "- HourlySeaLevelPressure - Sea level pressure given in inches of Mercury (in Hg).\n",
    "- HourlyStationPressure - Atmospheric pressure observed at the station during the time of observation. Given in inches of Mercury (in Hg).\n",
    "- HourlyVisibility - Horizontal distance an object can be seen and identified given in whole miles.\n",
    "- HourlyWetBulbTemperature - Wet-bulb temperature. It is given here in whole degrees Fahrenheit.\n",
    "- HourlyWindDirection - Wind direction from true north using compass directions. A direction of “000” is given for calm winds.\n",
    "- HourlyWindGustSpeed - Wind gusts occurring during time of observation. Given in miles per hour (mph).\n",
    "- HourlyWindSpeed - Speed of the wind at the time of observation given in miles per hour (mph).\n",
    "- bad_weather_frequency - Frequency of bad weather events as a percentage, grouped by airport.\n",
    "\n",
    "One-hot encoded features:\n",
    "\n",
    "- HourlySkyConditions: one-hot encoded feature that contains 3 character abbreviation of sky conditions.\n",
    "  - CLR (clear sky)\n",
    "  - FEW (few clouds)\n",
    "  - SCT (scattered clouds)\n",
    "  - BKN (broken clouds)\n",
    "  - OVC (overcast)\n",
    "  - VV (obscured sky)\n",
    "  - 10 (partially obscured sky)\n",
    "\n",
    "- HourlyPresentWeatherType: one hot encoded feature that contains 2 character abbreviation of weather type.\n",
    "  - DZ:01 - Drizzle\n",
    "  - RA:02 - Rain\n",
    "  - SN:03 - Snow\n",
    "  - SG:04 - Snow Grains\n",
    "  - IC:05 - Ice Crystals\n",
    "  - PL:06 - Ice Pellets\n",
    "  - GR:07 - Hail\n",
    "  - GS:08- Small Hail an/or Snow Pellets\n",
    "  - UP:09 - Unknown Precipitation\n",
    "  - BR:1 - Mist\n",
    "  - FG:2 - Fog\n",
    "  - FU:3 - Smoke\n",
    "  - VA:4 - Volcanic Ash\n",
    "  - DU:5 - Widespread Dust\n",
    "  - SA:6 - Sand\n",
    "  - HZ:7 - Haze\n",
    "  - PY:8 - Spray\n",
    "  - PO:1 - Well developed dust/sand whirls\n",
    "  - SQ:2 - Squalls\n",
    "  - FC:3 - Funnel CLoud, Waterspout or Tornado\n",
    "  - SS:4 - Sandstorm\n",
    "  - DS:5 - Duststorm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c25df202-0382-4415-8db5-35d052b0e99b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering, Refinement and Visualization of EDA\n",
    "\n",
    "We have around a 15% delay rate on regular days and for some holiday seasons, the flight delay rate may surge up to 30% to 35% so to depict this phenomenon, we created a new feature, is_holiday_period to label out if it's in the range of holiday period. Besides the holiday period, we also calculated the delay frequency by airports to reflect the situation of airport maintenance. Lastly, we defined the bad weather by visibility, wind speed, and other weather conditions by the airport to extract information about weather that may potentially induce delayed flights or cancellations. The table of feature engineering and refinement is as follows.  \n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/RyanChen12035/w261_team_6_1/blob/main/feature_engineering_refinement_1.PNG?raw=true\" width=\"1000\">\n",
    "<br>\n",
    "<img src=\"https://github.com/RyanChen12035/w261_team_6_1/blob/main/feature_engineering_refinement_.PNG?raw=true\" width=\"1000\">\n",
    "<br>\n",
    "\n",
    "After feature engineering, we then advanced to check out the correlations between features and the target variable. There is mild collinearity observed between features, and we planned to fix it by applying PCA where orthogonal principals are helpful in training MLP and preventing overfitting caused by collinearity.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://dchansew.github.io/w261/Phase 3 - Correlation Matrix.png\" width=\"750\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34c296e7-65fb-4afc-b16c-301f6760c6c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "Originally, we had 203 features after the EDA process and we then boiled it down with PCA to 12 features for MLP since dimension reduction and noise reduction are beneficial to Multi-layer Perceptron(MLP) training. Furthermore, we noticed there is mild multicollinearity between features after the EDA process, and the PCA process can help fix it with orthogonal principles. After the PCA process, the information within features is more spread out as below. The correlation between DEP_TIME and the target variable is 0.23. However, the highest correlation in PCA features is merely -0.12. One thing worth being aware of is that even though noise reduction may be beneficial to MLP training, it can also toss away some useful information, and we will talk about it in the MLP training section.\n",
    "<br>\n",
    "<img src=\"https://dchansew.github.io/w261/Phase 3 - PCA.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42b6c613-34f7-4a59-a103-27d2e6ac0411",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Information Leakage \n",
    "\n",
    "As we were training the logistic regression model, the accuracy suddenly shot up to 98%, so we noticed there was information leakage. As a result, we delved into each feature and based on the principle that the prediction should be made two hours before departure by the request of this project, we removed features belonging to the future information. For example, we can only know the arrival delay when the flight arrives at the destination airport so if we adapt this feature, we are at risk of information leakage. Same reason for the features of TAXI_IN/OUT, and WHEELS_IN/OFF. As for the feature of CARRIER_DELAY, it's the delay caused by the airline company. Since we have a similar feature to describe the reputation of the airline company and it's hard to know two hours prior departure, We decided to remove it as well. However, CARRIER_DELAY has a high correlation to our target variable and it's insightful in predicting so though we prevented our model from information leakage, we somehow lost some important information at the same time. Inevitably, it interfered with the performance of our models and we will talk about it in the following section.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/RyanChen12035/w261_team_6_1/blob/main/information_leakage.PNG?raw=true\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d38ca97d-5b9b-4174-98f5-bb626be54532",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Modeling Pipelines\n",
    "\n",
    "Firstly, we encapsulated our models with Pyspark pipeline and took customized data preparation steps corresponding to each model. For the logistic regression model, we took normalization on both ordinal and numeric features and concatenated it with one-hot encoded features and for the decision trees, we can straightly adapt data after EDA since it wouldn't be affected by the data type or scale. Lastly, for the MLP model, since PCA is beneficial for MLP training by removing dimension and noise, we generated 12 PCA features and fed them to the MLP model.Next, we split the data of 60M and adapted data from 2015 to 2018 to train the model and data of 2019 as testing data. With the time-series cross-validation, we defined in phase 2, we ran the grid search for the logistic regression model, gradient-boosted Decision Tree, and random forest to find the best hyperparameter set and as for MLP, we manually tested out the best structure for the task. The Detailed hyperparameters are listed below.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/RyanChen12035/w261_team_6_1/blob/main/pipeline_hyperparameters.PNG?raw=true\" width=\"1000\">\n",
    "<br>\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "In training process, we noticed there was information leakage, and we fixed it by removing future peeking features. The hyperparameters of the logistic regression model here are elasticNetParam which means the ratio of L1 and L2  and regParam which means L1 respectively, and we would find the best set in the grid search stage. As for the batch size, loss function, and optimizer, the default settings of the logistic regression model in Pyspark are full batch training, binary cross-entropy, and L-BFGS respectively.\n",
    "\n",
    "Binary Cross-Entropy Loss Function with L1 and L2 Regularization:\n",
    "$$ L = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] + \\lambda \\sum_{j=1}^{m} |w_j| + \\lambda \\sum_{j=1}^{m} w_j^2 $$\n",
    "\n",
    "Lastly, we got test accuracy and recall of 81.6% and precision of 66.6% on data of 60M and surprisingly found out the performance is at the same level as the performance of the decision trees and it may be attributed to imbalance distribution of the target variable. We will have a further discussion in the following section. It takes around 27 minutes to fit in the model using Standard-D4ds-16GB-4cores and during the process, the worker number fluctuated in the range of 4 to 6. Also, it takes more than 6 hours to run grid search with time-series cross-validation.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/RyanChen12035/w261_team_6_1/blob/main/logistic_regression_experiements.PNG?raw=true\" width=\"1000\">\n",
    "<br>\n",
    "\n",
    "#### Gradient-boosted Decision Trees\n",
    "\n",
    "Gradient boosting is ensemble-based additive modeling technique that fits a tree to the problem, then generates predictions with that tree. It then subtracts a small amount of the tree's prediction from the original regression labels and the next tree gets trained on the leftovers. For classification problems, the predicted class for an observation is the class that yields the largest weighted average of the class posterior probabilities using selected trees.\n",
    "\n",
    "Logistic loss function for GBTClassifier:\n",
    "$$\n",
    "\\text{Logistic Loss} = -\\sum_{i=1}^{n} \\left[ y_{\\text{true}}^{(i)} \\cdot \\log(p_{\\text{pred}}^{(i)}) + (1 - y_{\\text{true}}^{(i)}) \\cdot \\log(1 - p_{\\text{pred}}^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "Gini Impurity for RandomForestClassifier:\n",
    "$$\n",
    "\\text{Gini Impurity} = 1 - \\sum_{i=1}^{C} (p_i)^2\n",
    "$$\n",
    "\n",
    "The Gini index is differentiable and amenable to numerical optimization. Additionally, the Gini index is sensitive to changes in node probabilities and generates higher purity when splitting the tree.\n",
    "\n",
    "It takes around 13 minutes to fit in both Random Forest and Gradient Boosted models using Standard-D4ds-16GB-4cores and during the process, the worker number fluctuated in the range of 4 to 6. Also, it takes more than 6 hours to run grid search with time-series cross-validation.\n",
    "\n",
    "For grid search, we used 1, 3, 5, 10 and 0.1, 0.3, 0.5 for max depth and step size hyper-parameters, respectively. The predictions smooth out relatively quickly and the ideal hyper-parameters were 3 and 0.1 for max depth and step size over the 12M dataset.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/RyanChen12035/w261_team_6_1/blob/main/gradient_boosted_trees_experiements.PNG?raw=true\" width=\"1000\">\n",
    "<br>\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "The random forest ensemble method produces trees based on the subsets of features and combines the output of multiple trees into a single result. It is also highly flexible and can handle classification and regression problems. \n",
    "\n",
    "For our grid search we used 20, 40, 60, 80 and 1, 3, 5, 10 for number of trees and max depth hyper-parameters, respectively. \n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/RyanChen12035/w261_team_6_1/blob/main/random_forest_experiements.PNG?raw=true\" width=\"1000\">\n",
    "<br>\n",
    "\n",
    "Although we had relatively high wall times, testing both ensemble mathods for decision trees with a diversity of stopping criteria and allowed us to select an ideal model and set of hyper-parameters.\n",
    "\n",
    "#### Multi-layer Perceptron\n",
    "\n",
    "Multi-layer Perceptron (MLP) is a feedforward neural network with at least 3 layers and the final structure we adapted is 178,128,64,8,4,2 with a declining number of perceptrons by layer. We got test accuracy and recall of 82% and precision of 67% when we applied PCA features as input. Surprisingly, we got a small hop to the precision of 70% by applying input features with only normalization on ordinal and numeric features. Even though dimension reduction and noise reduction by PCA are beneficial to MLP training, we can not identify noise from useful information in this case. Some useful information that can help with our model to improve the performance could be tossed away during the PCA process. As a result, we adapted input data without PCA and trained the MLP model on data of 60M. There were no changes in model performance between the 12M and 60M test runs.\n",
    "\n",
    "Binary logistic loss function for the MLP:\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i=1}^{n} \\left[ y_{\\text{true}}^{(i)} \\cdot \\log(y_{\\text{pred}}^{(i)}) + (1 - y_{\\text{true}}^{(i)}) \\cdot \\log(1 - y_{\\text{pred}}^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "It takes around 32.6 minutes to fit in both Random Forest and Gradient Boosted models using Standard-D4ds-16GB-4cores and during the process, the worker number fluctuated in the range of 4 to 6.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"https://dchansew.github.io/w261/Phase 3 - MLP 2.png\"width=\"1000\">\n",
    "<br>\n",
    "\n",
    "Reference: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2798664673130479/command/2798664673130480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b40626d-a480-4e9e-b69a-c3b89737762b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Imbalanced Target Variable\n",
    "\n",
    "We noticed that we have the same result coming out from the Logistic Regression and Random Forest Tree and it made us think if it's caused by an imbalanced target variable. To figure it out, We sampled Non-delay data based on the size of the Delay data. The detailed experiment setup is as follows. The accuracy and recall increase while the proportion of Non-delay increases suggesting the ability to identify Non-delay is increasing. However, the precision remains at the same level suggesting the models are getting better at identifying Non-delay data but not getting better at identifying delay data. Hence, we can know that the model's performance in predicting delays is not severely affected by the prevalence of the Non-delays in the Non-delay data proportion range from 2 to 6. When the ratio increases up to 2 to 8, the precision drops 4% suggesting Non-delay data now slightly affects the ability of the model to identify delayed flights. Furthermore, the information provided by delay data is not enough so the precision stuck at the level of around 70%. \n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/RyanChen12035/w261_team_6_1/blob/main/Imbalanced_target_variable.PNG?raw=true\" width=\"1000\">\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fd38217-a96a-4ad4-ac19-361164a62f48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Results and Gap Analysis\n",
    "\n",
    "In the trial experiments with the logistic regression model, we noticed that there was an information leakage issue. The accuracy shot up to 98% because of some overly insightful features, and to handle this issue, we examined all the features one by one, and based on the principle that the prediction has to be made two hours prior, we removed the features that have a risk of future peeking. Next, the Logistic regression achieves accuracy accuracy and recall of 81% and precision of 74% on data of 12M but the precision drops to 66% after applying data of 60M. Same for the Gradient-boosted Decision Trees, the initial accuracy and recall are both 82%, it's slightly higher than the Logist Regression model. However, the precision also dropped from 78% to 67% when we fed in more data and we think it may be partly attributed to the fact we have an imbalanced target variable on data of 60M. The level of non-delay data affects the prediction of the model in data of 60M can be more severe than data of 12M. Besides, there may be events crossing years that can potentially affect the prediction such as ENSO which can change the weather conditions every four years. To further improve the performance of our models, we need to label these events in future work. As for the Multi-layer Perceptron model, it achieves both test accuracy and recall of 82% on data of 60M and the precision trained by PCA features is 67% as well, but it can be slightly improved by feeding in data without PCA suggesting the noise removed by PCA can be useful information that can be extracted by MLP models to improve the performance.\n",
    "\n",
    "Lastly, Both Logistic Regression model and Gradient-boosted Decision Trees have encountered a hard ceiling for the performance of precision at a level of 70% and it's attributed to the fact that the information provided by the delayed data is not enough for the models to learn the pattern inside of it and make the right prediction. As for the Multi-layer Perceptron model, it achieves both accuracy and recall of 82% on data of 60M. However, the precision also sticks at the level of 70%. To handle this issue, we need a more explicit feature engineering to extract information from delay data, or a more complicated model to tackle this situation. Also, it somehow echos the way we prevent our model from information leakage by removing some insightful features. Should we adapt the features of CARRIER_DELAY, WEATHER_DELAY, and origin_icao_delay we removed to gain better performance? If they are against the rules of future peeking, perhaps we can calculate the historic stats to extract information such as the average CARRIER_DELAY by an airline company to improve the precision of the models. After all, if the model makes a wrong prediction, it would mess up the customer's schedule so we need to boost the precision as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc44f1b2-6533-4d5b-b266-28043a292837",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "Airline flight delays can have a significant impact for passengers and the airline industry, leading to inconveniences, increased operational costs and decreased customer satisfaction. These delays create logistical issues that ultimately can impact revenues and increase the cost of air travel as a whole. The financial and mental repercussions of airline travel delays is an important pain point for airline carriers, airports and travelers. With the rich amount of available airline travel and weather data, our ML pipeline can forecast significant departure delays for a given flight – a highly performant model can assist airline carrier and airports to better manage flight schedules and destinations and reduce risk of travel delays. Through exploratory data analysis, feature engineering, iteration and testing, we identified a model using Multi-layer Perceptron(MLP) that achieves an optimal accuracy of 82% with a recall of 82%. Given the results of our testing on target variable class imbalances, we believe there is an opportunity to improve our 70% precision by combining an optimally balanced dataset and additional feature engineering such as historical average carrier delay. We believe these improvements will yield a final model that can be deployed at enterprise scale. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ecb3e4f-9402-4b30-9264-f15522bef9f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "| Name             | Workload                                                                                                                   |\n",
    "|------------------|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Jason Rudianto   | Presentation, EDA - Flights, Model Pipeline, Class Imbalance, MLP                                                                                 |\n",
    "| Derrick Chan-Sew | Presentation, Model Pipeline, Visualizations, Decision Tree - Gradient-boosted                                                                  |\n",
    "| Sean Condon      | Presentation, EDA - Weather, Abstract, Model Pipeline, Decision Tree - Random Forest                                                                       |\n",
    "| Ryan Chen        | Presentation, EDA - Stations, checkpoint, Model Pipeline, Logistic Regression, Class Imbalance                                     |\n",
    "| Jessica Huber    | Presentation, Model Pipeline, Logistic/Linear Regression Baseline, MLP                                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d16a487-b03a-4971-a68b-1e33f2651ddd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "%md\n",
    "\n",
    "# Reference\n",
    "\n",
    "Notebooks:<br>\n",
    "EDA on 60M:https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319520334/command/1346418319520335<br>\n",
    "PCA:https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319518436/command/1346418319518437<br>\n",
    "Visualization of feature engineering: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319522370/command/1346418319522371<br>\n",
    "logistic regression on 60M: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319515964/command/1346418319516249<br>\n",
    "MLP on 60M: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319520186<br>\n",
    "GBDT & RF on 12M: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319519548/command/1346418319519549 <br>\n",
    "GBDT & RF: on 60M: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1346418319522848/command/1346418319522849\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FP Phase 3 - Team 6-1 Final Deliverable_final_final",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
